{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env(env, agent, timesteps=1000, averaging_steps=100, progress_bar=True):\n",
    "    env.reset()\n",
    "    agent.reset()\n",
    "    mean_reward = 0\n",
    "\n",
    "    iterator = tqdm(range(timesteps)) if progress_bar else range(timesteps)\n",
    "    for i in iterator:\n",
    "        action = agent.act()\n",
    "        reward = env.step(action)\n",
    "        agent.update(action, reward)\n",
    "\n",
    "        if i == timesteps - averaging_steps:\n",
    "            mean_reward += 1/(i - timesteps + averaging_steps + 1)*(reward - mean_reward)\n",
    "\n",
    "    return mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100/100 [00:01<00:00, 96.07it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 91.50it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.42it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 96.87it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 97.96it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 98.31it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.41it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.23it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.04it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.04it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.49it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.82it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'UCBAgent' object has no attribute 'alpha'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9339f8c36942>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mmean_rw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maveraging_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mmean_mean_rw\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_rw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_mean_rw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d27e631490a8>\u001b[0m in \u001b[0;36mrun_env\u001b[0;34m(env, agent, timesteps, averaging_steps, progress_bar)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtimesteps\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0maveraging_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Programming/Machine-Learning/sutton-barto/chapter-2/utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, action, reward)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UCBAgent' object has no attribute 'alpha'"
     ]
    }
   ],
   "source": [
    "mean_alpha_fun = lambda x: 1/x\n",
    "k = 10\n",
    "spec = {'eps-greedy-mean': (np.linspace(1/128, 1/4, 6), lambda cur_eps: QAgent(alpha=mean_alpha_fun, k=k, eps=cur_eps)),\n",
    "        'gradient-bandit': (np.linspace(1/32, 3, 6), lambda cur_alpha: GradientBandits(k=k, alpha=cur_alpha)),\n",
    "        'UCB': (np.linspace(1/16, 4, 6), lambda cur_c: UCBAgent(k=k, c=cur_c)),\n",
    "        'eps-greedy-const': (np.linspace(1/128, 1/4, 6), lambda cur_eps: QAgent(alpha=0.1, k=k, eps=cur_eps)),\n",
    "        'greedy-optimistic': (np.linspace(1/4, 4, 6), lambda Q_0: QAgent(alpha=0.1, k=k, initial_Q=np.full(k, Q_0)))}\n",
    "\n",
    "mean_rewards = {'eps-greedy-mean': [],\n",
    "                'gradient-bandit': [],\n",
    "                'UCB': [],\n",
    "                'eps-greedy-const': [],\n",
    "                'greedy-optimistic': []}\n",
    "for agent_desc, (space, agent_fun) in spec.items():\n",
    "    for val in space:\n",
    "\n",
    "        agent = agent_fun(val)\n",
    "        env = KBandits(k=k, stationary=True)\n",
    "        mean_mean_rw = 0\n",
    "\n",
    "        for i in tqdm(range(100)):\n",
    "            mean_rw = run_env(env, agent, timesteps=1000, averaging_steps=100, progress_bar=False)\n",
    "            mean_mean_rw += 1/(i+1)*(mean_rw - mean_mean_rw)\n",
    "\n",
    "        mean_rewards[agent_desc].append(mean_mean_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_desc, (space, _) in spec.items():\n",
    "    plt.plot(space, mean_rewards[agent_desc], label=agent_desc)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylabel('Average reward over last 100 steps')\n"
   ]
  }
 ]
}